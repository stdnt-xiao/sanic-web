services:
  chat-web:
    image: crpi-7xkxsdc0iki61l0q.cn-hangzhou.personal.cr.aliyuncs.com/apconw/chat-vue3-mvp:1.1.9
    container_name: chat-vue3-mvp
    environment:
      DOCKER_HOST_INTERNAL: ${DOCKER_HOST_INTERNAL:-}
      TZ: Asia/Shanghai
    volumes:
      - ./nginx.conf.template:/etc/nginx/conf.d/nginx.conf.template
    ports:
      - "8081:80"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - chat-service

  minio:
    image: minio/minio:RELEASE.2025-04-22T22-12-26Z
    container_name: minio
    ports:
      - "19000:9000"
      - "19001:9001"
    volumes:
      - ./volume/minio/data:/data
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=12345678
    command: server /data --console-address ":9001"

  chat-service:
    image: crpi-7xkxsdc0iki61l0q.cn-hangzhou.personal.cr.aliyuncs.com/apconw/sanic-web:1.1.9
    container_name: sanic-web
    environment:
      SERVER_PORT: ${SERVER_PORT:-8088}
      SERVER_WORKERS: ${SERVER_WORKERS:-2}
      MYSQL_HOST: ${MYSQL_HOST:-}
      MYSQL_PORT: ${MYSQL_PORT:-}
      MYSQL_USER: ${MYSQL_USER:-}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-}
      MYSQL_DATABASE: ${MYSQL_DATABASE:-}
      SQLALCHEMY_DATABASE_URI: ${SQLALCHEMY_DATABASE_URI:-}
      DIFY_SERVER_URL: ${DIFY_SERVER_URL:-}
      DIFY_DATABASE_QA_API_KEY: ${DIFY_DATABASE_QA_API_KEY:-}
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-}:${MINIO_PORT:-}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-}
      MiNIO_SECRET_KEY: ${MiNIO_SECRET_KEY:-}
      MODEL_BASE_URL: ${MODEL_BASE_URL:-}
      MODEL_NAME: ${MODEL_NAME:-}
      MODEL_TEMPERATURE: ${MODEL_TEMPERATURE:-}
      MODEL_API_KEY: ${MODEL_API_KEY:-}
      MCP_HUB_COMMON_QA_GROUP_URL: ${MCP_HUB_COMMON_QA_GROUP_URL:-}
      MCP_HUB_DATABASE_QA_GROUP_URL: ${MCP_HUB_DATABASE_QA_GROUP_URL:-}
      SHOW_THINKING_PROCESS: ${SHOW_THINKING_PROCESS:-}
      NEO4J_URI: ${NEO4J_URI:-}
      NEO4J_USER: ${NEO4J_USER:-}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-}
      JWT_SECRET_KEY: ${JWT_SECRET_KEY:-}
      TZ: Asia/Shanghai
    ports:
      - "8088:8088"
    extra_hosts:
      - "host.docker.internal:host-gateway"

  mysql:
    image: mysql:latest
    container_name: chat-db
    ports:
      - "13006:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=1
      - CHARACTER_SET_SERVER=utf8mb4
      - COLLATION_SERVER=utf8mb4_unicode_ci
      - TZ=Asia/Shanghai
    volumes:
      - ./volume/mysql/data:/var/lib/mysql
      - ./my.cnf:/etc/mysql/conf.d/my.cnf

  mcphub:
    image: samanhappy/mcphub:0.9.12
    container_name: mcphub
    ports:
      - "3300:3000"
    volumes:
      - ./volume/mcp-data/mcp_settings.json:/app/mcp_settings.json
    extra_hosts:
      - "host.docker.internal:host-gateway"

  gpt-vis-api:
    image: crpi-7xkxsdc0iki61l0q.cn-hangzhou.personal.cr.aliyuncs.com/apconw/gpt-vis-api:0.0.1
    container_name: gpt-vis-api
    ports:
      - "3100:3000"
    environment:
      MINIO_USE_SSL: "false"
      MINIO_PORT: ${MINIO_PORT:-}
      MINIO_ENDPOINT: ${MINIO_ENDPOINT:-}
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-}
      MINIO_SECRET_KEY: ${MiNIO_SECRET_KEY:-}
      MINIO_BUCKET: ${MINIO_BUCKET:-}
      MINIO_PUBLIC_DOMAIN: ${MINIO_PUBLIC_DOMAIN:-}
    extra_hosts:
      - "host.docker.internal:host-gateway"

  neo4j-apoc:
    image: neo4j:5.26.11-ubi9
    container_name: neo4j-apoc
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - ./volume/neo4j/data:/data
      - ./volume/neo4j/plugins:/plugins
    environment:
      - apoc.export.file.enabled=true
      - apoc.import.file.enabled=true
      - apoc.import.file.use_neo4j_config=true
      - NEO4J_AUTH=neo4j/neo4j123

  mineru-vllm-server:
    image: mineru-vllm:latest
    container_name: mineru-vllm-server
    restart: always
    profiles: ["vllm-server"]
    ports:
      - 30000:30000
    environment:
      MINERU_MODEL_SOURCE: local
    entrypoint: mineru-vllm-server
    command:
      --host 0.0.0.0
      --port 30000
      # --data-parallel-size 2  # If using multiple GPUs, increase throughput using vllm's multi-GPU parallel mode
      # --gpu-memory-utilization 0.5  # If running on a single GPU and encountering VRAM shortage, reduce the KV cache size by this parameter, if VRAM issues persist, try lowering it further to `0.4` or below.
    ulimits:
      memlock: -1
      stack: 67108864
    ipc: host
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:30000/health || exit 1"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

  mineru-api:
    image: mineru-vllm:latest
    container_name: mineru-api
    restart: always
    profiles: ["api"]
    ports:
      - 8000:8000
    environment:
      MINERU_MODEL_SOURCE: local
    entrypoint: mineru-api
    command:
      --host 0.0.0.0
      --port 8000
      # parameters for vllm-engine
      # --data-parallel-size 2  # If using multiple GPUs, increase throughput using vllm's multi-GPU parallel mode
      # --gpu-memory-utilization 0.5  # If running on a single GPU and encountering VRAM shortage, reduce the KV cache size by this parameter, if VRAM issues persist, try lowering it further to `0.4` or below.
    ulimits:
      memlock: -1
      stack: 67108864
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ "0" ]
              capabilities: [ gpu ]

  mineru-gradio:
    image: mineru-vllm:latest
    container_name: mineru-gradio
    restart: always
    profiles: ["gradio"]
    ports:
      - 7860:7860
    environment:
      MINERU_MODEL_SOURCE: local
    entrypoint: mineru-gradio
    command:
      --server-name 0.0.0.0
      --server-port 7860
      --enable-vllm-engine true  # Enable the vllm engine for Gradio
      # --enable-api false  # If you want to disable the API, set this to false
      # --max-convert-pages 20  # If you want to limit the number of pages for conversion, set this to a specific number
      # parameters for vllm-engine
      # --data-parallel-size 2  # If using multiple GPUs, increase throughput using vllm's multi-GPU parallel mode
      # --gpu-memory-utilization 0.5  # If running on a single GPU and encountering VRAM shortage, reduce the KV cache size by this parameter, if VRAM issues persist, try lowering it further to `0.4` or below.
    ulimits:
      memlock: -1
      stack: 67108864
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ "0" ]
              capabilities: [ gpu ]
